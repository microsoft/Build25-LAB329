{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13490c0a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Knowledge Distillation: Generating Training Data from a Teacher Model\n",
    "\n",
    "This notebook demonstrates the first step in knowledge distillation: generating high-quality training data from a large \"teacher\" model. Knowledge distillation is a technique where a smaller, more efficient model learns from a larger, more powerful model rather than directly from raw data.\n",
    "\n",
    "![](../../lab_manual/images/step-1.png)\n",
    "\n",
    "## What You'll Learn\n",
    "- How to load and format multiple-choice questions\n",
    "- How to send these questions to a large language model (DeepSeek-V3)\n",
    "- How to collect and save the teacher model's responses\n",
    "- How to analyze the quality of generated training data\n",
    "\n",
    "## Prerequisites\n",
    "- Access to an Azure AI model endpoint (DeepSeek-V3)\n",
    "- Azure credentials set up in a local.env file\n",
    "- Python environment with necessary libraries\n",
    "\n",
    "## Setup Instructions\n",
    "1. **Azure Authentication**: Ensure you're logged in to Azure using `az login --use-device-code` in a terminal\n",
    "2. **Kernel Selection**: Change the Jupyter kernel to **\"Python 3.10 AzureML\"** using the selector in the top right\n",
    "3. **Environment File**: Ensure your `local.env` file exists with proper credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585941a1",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, we'll install the packages needed for this notebook. The `dotenv-azd` package allows us to securely load environment variables from Azure Developer CLI (AZD) environments and/or local .env files. This is how we'll access our Azure credentials without hardcoding sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddced75",
   "metadata": {
    "gather": {
     "logged": 1745565408411
    }
   },
   "outputs": [],
   "source": [
    "pip install dotenv-azd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f85a53",
   "metadata": {},
   "source": [
    "## 2. Install Data Handling Libraries\n",
    "\n",
    "Next, we install two important packages:\n",
    "\n",
    "- `datasets`: Hugging Face's library that provides easy access to thousands of publicly available datasets\n",
    "- `tqdm`: A progress bar library that helps visualize long-running operations\n",
    "\n",
    "The `-U` flag ensures we get the latest versions of these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb8ff3",
   "metadata": {
    "gather": {
     "logged": 1745565221871
    }
   },
   "outputs": [],
   "source": [
    "pip install datasets tqdm -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ebfbd",
   "metadata": {},
   "source": [
    "## 3. Import Dataset Libraries\n",
    "\n",
    "Now we import the libraries needed to work with our datasets:\n",
    "\n",
    "- `load_dataset`: A function from Hugging Face that lets us download and use public datasets\n",
    "- `ABC` (Abstract Base Class): A Python feature that helps us create a framework for our dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9605ccc",
   "metadata": {
    "gather": {
     "logged": 1745565223801
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from abc import ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c51934",
   "metadata": {},
   "source": [
    "## 4. Create Base Dataset Class\n",
    "\n",
    "Here we define our base class for handling datasets. This creates a common structure that all our dataset handlers will follow:\n",
    "\n",
    "1. We're using Python's Abstract Base Class (`ABC`) as a foundation\n",
    "2. The class initializes three important attributes that will track our data files:\n",
    "   - `train_data_file_name`: For our training data\n",
    "   - `test_data_file_name`: For our test data\n",
    "   - `eval_data_file_name`: For our evaluation/validation data\n",
    "\n",
    "These file names are initially set to `None` and will be filled in later when we create our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f5e03",
   "metadata": {
    "gather": {
     "logged": 1745565224814
    }
   },
   "outputs": [],
   "source": [
    "class InputDataset(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        (\n",
    "            self.train_data_file_name,\n",
    "            self.test_data_file_name,\n",
    "            self.eval_data_file_name,\n",
    "        ) = (None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43455abd",
   "metadata": {},
   "source": [
    "## 5. Create Specialized Dataset Handler\n",
    "\n",
    "This class extends our base `InputDataset` to handle loading data specifically from Hugging Face. The `load_hf_dataset` method does the following:\n",
    "\n",
    "1. Loads a dataset from Hugging Face by name (e.g., \"tau/commonsense_qa\")\n",
    "2. Creates three different data splits:\n",
    "   - Training data: Used to generate examples for our model\n",
    "   - Validation data: Used to evaluate during training\n",
    "   - Test data: Used for final evaluation\n",
    "\n",
    "3. Controls how many examples we take from each split using the sample size parameters\n",
    "\n",
    "The code handles two scenarios:\n",
    "- Datasets that already have validation splits\n",
    "- Datasets that only have train/test splits (in which case it creates a validation set from the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34315645",
   "metadata": {
    "gather": {
     "logged": 1745565225277
    }
   },
   "outputs": [],
   "source": [
    "class CQnAHuggingFaceInputDataset(InputDataset):\n",
    "    \"\"\"\n",
    "    Loads the HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def load_hf_dataset(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        train_sample_size=10,\n",
    "        val_sample_size=10,\n",
    "        test_sample_size=10,\n",
    "        train_split_name=\"train\",\n",
    "        val_split_name=\"validation\",\n",
    "        test_split_name=\"test\",\n",
    "    ):\n",
    "        full_dataset = load_dataset(dataset_name)\n",
    "\n",
    "        if val_split_name is not None:\n",
    "            train_data = full_dataset[train_split_name].select(range(train_sample_size))\n",
    "            val_data = full_dataset[val_split_name].select(range(val_sample_size))\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "        else:\n",
    "            train_val_data = full_dataset[train_split_name].select(\n",
    "                range(train_sample_size + val_sample_size)\n",
    "            )\n",
    "            train_data = train_val_data.select(range(train_sample_size))\n",
    "            val_data = train_val_data.select(\n",
    "                range(train_sample_size, train_sample_size + val_sample_size)\n",
    "            )\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "\n",
    "        return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2eb02d",
   "metadata": {},
   "source": [
    "## 6. Load the CommonsenseQA Dataset\n",
    "\n",
    "Now we'll actually load our dataset using the classes we defined. Here's what this cell does:\n",
    "\n",
    "1. **Define sample sizes**: We're using 100 examples each for training and validation to keep this notebook running quickly\n",
    "\n",
    "2. **Choose a dataset**: We're using \"tau/commonsense_qa\", which contains multiple-choice questions that test common sense reasoning\n",
    "\n",
    "3. **Create an instance of our dataset handler** and use it to load the data\n",
    "\n",
    "4. **Print the dataset sizes** to confirm everything loaded correctly\n",
    "\n",
    "The CommonsenseQA dataset is perfect for knowledge distillation because it contains challenging questions with five possible answers (labeled A through E). This will test how well our model has learned to reason like the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92e682",
   "metadata": {
    "gather": {
     "logged": 1745565225856
    }
   },
   "outputs": [],
   "source": [
    "# We can define train and test sample sizes here. Validation size is kept same as test sample size\n",
    "train_sample_size = 100\n",
    "val_sample_size = 100\n",
    "\n",
    "# Sample notebook using the dataset: https://huggingface.co/datasets/tau/commonsense_qa\n",
    "dataset_name = \"tau/commonsense_qa\"\n",
    "input_dataset = CQnAHuggingFaceInputDataset()\n",
    "\n",
    "# Note: train_split_name and test_split_name can vary by dataset. They are passed as arguments in load_hf_dataset.\n",
    "# If validation_split_name is None, the below function will split the train set to create the specified sized validation set.\n",
    "train, val, _ = input_dataset.load_hf_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    train_sample_size=train_sample_size,\n",
    "    val_sample_size=val_sample_size,\n",
    "    train_split_name=\"train\",\n",
    "    val_split_name=\"validation\",\n",
    ")\n",
    "\n",
    "print(\"Len of train data sample is \" + str(len(train)))\n",
    "print(\"Len of validation data sample is \" + str(len(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2508d5",
   "metadata": {},
   "source": [
    "## 7. Create Data Directory\n",
    "\n",
    "This simple command creates a directory called `data` where we'll store our processed files. The `-p` flag ensures that:\n",
    "\n",
    "1. If the directory already exists, no error will occur\n",
    "2. If parent directories don't exist, they will be created automatically\n",
    "\n",
    "This is where we'll store our questions and the teacher model's answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a458770",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfaa69",
   "metadata": {},
   "source": [
    "## 8. Set Output File Path\n",
    "\n",
    "Here we define the path where we'll save our processed training data. The file will be called `train_original_data.jsonl` in the data directory we just created.\n",
    "\n",
    "The `.jsonl` extension indicates this is a JSON Lines file format - a convenient format for large datasets where each line is a valid JSON object. This format is useful because:\n",
    "\n",
    "- It allows processing one example at a time (streaming)\n",
    "- Each example is self-contained on its own line\n",
    "- It's human-readable and easily parsed by most programming languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25914972",
   "metadata": {
    "gather": {
     "logged": 1745565227374
    }
   },
   "outputs": [],
   "source": [
    "train_data_path = \"./data/train_original_data.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f4dc8",
   "metadata": {},
   "source": [
    "## 9. Import JSON Library\n",
    "\n",
    "Here we import the `json` module, which is part of Python's standard library. This module provides functions to:\n",
    "\n",
    "- Convert Python objects to JSON strings (`json.dumps()`)\n",
    "- Parse JSON strings into Python objects (`json.loads()`)\n",
    "- Write JSON data to files (`json.dump()`)\n",
    "- Read JSON data from files (`json.load()`)\n",
    "\n",
    "We'll use these functions to format and save our questions and answers in a structured way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f3072",
   "metadata": {
    "gather": {
     "logged": 1745565227781
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67827b",
   "metadata": {},
   "source": [
    "## 10. Format Questions for the Teacher Model\n",
    "\n",
    "This cell prepares our questions in the format required by the teacher model. Here's what happens step-by-step:\n",
    "\n",
    "1. **Define prompts**:\n",
    "   - A system prompt that tells the model to answer with only one letter (A, B, C, D, or E)\n",
    "   - A template for user messages that formats each question and its answer choices\n",
    "\n",
    "2. **Create/clear the output file** to ensure we start with a clean file\n",
    "\n",
    "3. **Process each example** in our training dataset:\n",
    "   - Extract the question and its multiple-choice options\n",
    "   - Format the answer choices with their labels (A, B, C, D, E)\n",
    "   - Create a message structure with system and user prompts\n",
    "   - Save each formatted example to our JSONL file\n",
    "\n",
    "4. **Add error handling** and progress tracking\n",
    "\n",
    "5. **Validate the file creation** by checking the first line\n",
    "\n",
    "The resulting file will contain all our questions formatted in a way that's ready to be sent to the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad051ac1",
   "metadata": {
    "gather": {
     "logged": 1745565228450
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "user_prompt_template = \"Answer the following multiple-choice question by selecting the correct option.\\n\\nQuestion: {question}\\nAnswer Choices:\\n{answer_choices}\"\n",
    "\n",
    "# First, create/truncate the file to ensure it's empty and exists\n",
    "with open(train_data_path, \"w\", encoding='utf-8') as f:\n",
    "    pass  # Just create the file, we'll append to it later\n",
    "\n",
    "print(f\"Generating training data and writing to {train_data_path}\")\n",
    "\n",
    "# Check if train data has expected structure\n",
    "if len(train) == 0:\n",
    "    print(\"Error: Train dataset is empty!\")\n",
    "else:\n",
    "    print(f\"Sample train data keys: {list(train[0].keys())}\")\n",
    "    \n",
    "    # Verify train data has the expected fields before processing\n",
    "    sample = train[0]\n",
    "    if 'question' in sample and 'choices' in sample:\n",
    "        for i, row in enumerate(train):\n",
    "            try:\n",
    "                data = {\"messages\": []}\n",
    "                data[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt,\n",
    "                    }\n",
    "                )\n",
    "                question, choices = row[\"question\"], row[\"choices\"]\n",
    "                labels, choice_list = choices[\"label\"], choices[\"text\"]\n",
    "                answer_choices = [\n",
    "                    \"({}) {}\".format(labels[i], choice_list[i]) for i in range(len(labels))\n",
    "                ]\n",
    "                answer_choices = \"\\n\".join(answer_choices)\n",
    "                data[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": user_prompt_template.format(\n",
    "                            question=question, answer_choices=answer_choices\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Write data as valid JSON line\n",
    "                with open(train_data_path, \"a\", encoding='utf-8') as f:\n",
    "                    f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                if i % 10 == 0:  # Show progress every 10 items\n",
    "                    print(f\"Processed {i+1}/{len(train)} questions\", end=\"\\r\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing row {i}: {str(e)}\")\n",
    "                print(f\"Row data: {row}\")\n",
    "        \n",
    "        print(f\"\\nSuccessfully wrote {len(train)} questions to {train_data_path}\")\n",
    "        \n",
    "        # Validate the file has content\n",
    "        with open(train_data_path, \"r\", encoding='utf-8') as f:\n",
    "            first_line = f.readline().strip()\n",
    "            print(f\"File validation: {'SUCCESS' if first_line else 'FAILED - Empty file'}\")\n",
    "            if first_line:\n",
    "                # Try parsing the first line to validate JSON\n",
    "                try:\n",
    "                    json_data = json.loads(first_line)\n",
    "                    print(f\"JSON validation: SUCCESS\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSON validation: FAILED - {str(e)}\")\n",
    "    else:\n",
    "        print(f\"Error: Train data doesn't have the expected structure. Found keys: {list(sample.keys())}\")\n",
    "        print(\"Expected 'question' and 'choices' keys in each sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063901e",
   "metadata": {},
   "source": [
    "## 11. Load Environment Variables\n",
    "\n",
    "This cell loads the configuration and credentials needed to connect to our teacher model. Instead of hardcoding sensitive information like API keys, we use environment variables for better security:\n",
    "\n",
    "1. `load_azd_env()`: Tries to load variables from an Azure Developer CLI (AZD) environment if available\n",
    "2. `load_dotenv()`: Falls back to loading from a local `.env` file\n",
    "\n",
    "This approach ensures our credentials remain secure and can be easily changed without modifying the code. The environment variables should include details for connecting to the Azure AI services where our teacher model is hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde7fd0",
   "metadata": {
    "gather": {
     "logged": 1745565229548
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv_azd import load_azd_env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from current AZD environment if available\n",
    "load_azd_env(quiet=True)\n",
    "\n",
    "# Load environment variables from local.env file if it exists\n",
    "load_dotenv(dotenv_path=\"local.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e2c3fd",
   "metadata": {},
   "source": [
    "## 12. Access Teacher Model Credentials\n",
    "\n",
    "Now we retrieve the specific credentials needed to access our teacher model from the environment variables. We need three key pieces of information:\n",
    "\n",
    "1. `teacher_model_name`: The name of the model we'll be using (e.g., \"DeepSeek-V3\")\n",
    "2. `teacher_model_endpoint_url`: The URL where the model API is hosted\n",
    "3. `teacher_model_api_key`: The authentication key to access the service\n",
    "\n",
    "We print confirmation that these values were loaded (without revealing the actual API key for security reasons). This helps verify our setup is correct before proceeding.\n",
    "\n",
    "Make sure these environment variables are set in your `local.env` file or AZD environment before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fc57b",
   "metadata": {
    "gather": {
     "logged": 1745565229865
    }
   },
   "outputs": [],
   "source": [
    "# Get Azure AI Foundry credentials from environment variables\n",
    "teacher_model_name = os.getenv('TEACHER_MODEL_NAME')\n",
    "teacher_model_endpoint_url = os.getenv('TEACHER_MODEL_ENDPOINT')\n",
    "teacher_model_api_key = os.getenv('TEACHER_MODEL_KEY')\n",
    "\n",
    "# Print values for debugging (remove in production)\n",
    "print(f\"Teacher Model Name: {teacher_model_name}\")\n",
    "print(f\"Teacher Model Endpoint: {teacher_model_endpoint_url}\")\n",
    "# Don't print the API key for security reasons\n",
    "print(f\"Teacher Model API Key loaded: {'Yes' if teacher_model_api_key else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d134e",
   "metadata": {},
   "source": [
    "## 13. Install Azure AI Inference SDK\n",
    "\n",
    "Here we install the `azure-ai-inference` package, which is Microsoft's official SDK for interacting with Azure AI services. This library provides:\n",
    "\n",
    "1. Client classes to connect to Azure AI models\n",
    "2. Methods to send prompts and receive responses\n",
    "3. Classes to structure messages in the expected chat format\n",
    "4. Functionality to set parameters like temperature and token limits\n",
    "\n",
    "This modern API makes it easy to communicate with large language models hosted on Azure AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052dde2",
   "metadata": {
    "gather": {
     "logged": 1745565232660
    }
   },
   "outputs": [],
   "source": [
    "pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa058f",
   "metadata": {},
   "source": [
    "## 14. Import Azure AI and Utility Libraries\n",
    "\n",
    "Now we import the libraries we'll need to communicate with the teacher model:\n",
    "\n",
    "1. **Azure AI Inference Classes**:\n",
    "   - `ChatCompletionsClient`: The main client for sending requests to the model\n",
    "   - `SystemMessage` and `UserMessage`: Classes to format our prompts correctly\n",
    "   - `AzureKeyCredential`: For authentication with our API key\n",
    "\n",
    "2. **Utility Libraries**:\n",
    "   - `json`: For working with JSON data (imported earlier)\n",
    "   - `os`: For accessing environment variables\n",
    "   - `tqdm`: For showing progress bars during processing\n",
    "\n",
    "These libraries give us everything we need to send our questions to the teacher model and process its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb1cbf",
   "metadata": {
    "gather": {
     "logged": 1745565233019
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa428a6",
   "metadata": {},
   "source": [
    "## 15. Define Question Processing Function\n",
    "\n",
    "This function handles sending a single question to the teacher model and processing its response. Here's how it works:\n",
    "\n",
    "1. **Create message objects** from our JSON data:\n",
    "   - Converts system prompts to `SystemMessage` objects\n",
    "   - Converts user prompts to `UserMessage` objects\n",
    "\n",
    "2. **Sends the messages** to the teacher model through the Azure AI client\n",
    "\n",
    "3. **Processes the response** and formats it for our training data\n",
    "\n",
    "4. **Handles errors** gracefully by returning an error message if something goes wrong\n",
    "\n",
    "The function returns a dictionary containing:\n",
    "- The original question\n",
    "- The model's response (answer)\n",
    "- The full response object (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f12ae7",
   "metadata": {
    "gather": {
     "logged": 1745565234026
    }
   },
   "outputs": [],
   "source": [
    "def process_question(question_data):\n",
    "    try:\n",
    "        messages = []\n",
    "        for msg in question_data[\"messages\"]:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                messages.append(UserMessage(content=msg[\"content\"]))\n",
    "\n",
    "        response = client.complete(\n",
    "            messages=messages,\n",
    "            model=model_name,\n",
    "            max_tokens=10000  # Reduced since we just need short answers like A, B, C, D, or E\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"question\": question_data[\"messages\"][1][\"content\"],\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"full_response\": response\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"question\": question_data[\"messages\"][1][\"content\"] if len(question_data[\"messages\"]) > 1 else \"Error\",\n",
    "            \"response\": f\"Error: {str(e)}\",\n",
    "            \"full_response\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8e4688",
   "metadata": {},
   "source": [
    "## 16. Initialize Azure AI Client\n",
    "\n",
    "Now we create the Azure AI client that will connect to our teacher model. This client handles all communication with the Azure AI service:\n",
    "\n",
    "1. **Set up the endpoint URL** using our environment variable\n",
    "2. **Set the model name** to specify which model to use\n",
    "3. **Create the client** with our endpoint and authentication credentials\n",
    "\n",
    "This client will be used by our `process_question` function to send questions to the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b00cf",
   "metadata": {
    "gather": {
     "logged": 1745565234583
    }
   },
   "outputs": [],
   "source": [
    "endpoint = f\"{teacher_model_endpoint_url}\"\n",
    "model_name = teacher_model_name\n",
    "key = teacher_model_api_key\n",
    "client = ChatCompletionsClient(endpoint=endpoint, credential=AzureKeyCredential(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9d6a9",
   "metadata": {},
   "source": [
    "## 17. Process Questions with the Teacher Model\n",
    "\n",
    "This section contains the main processing loop that sends our formatted questions to the teacher model. Here's what this code does:\n",
    "\n",
    "1. **Validates the input file** to ensure it exists and has valid JSON content\n",
    "\n",
    "2. **Sets up a progress bar** to track processing in real-time\n",
    "\n",
    "3. **Processes each question** by:\n",
    "   - Reading the question data from the file\n",
    "   - Sending it to our `process_question` function\n",
    "   - Adding the result to our results list\n",
    "   - Updating the progress bar\n",
    "\n",
    "4. **Handles errors** that might occur during processing\n",
    "\n",
    "5. **Reports overall statistics** at the end\n",
    "\n",
    "This is the core of our knowledge distillation process - we're collecting expert knowledge from the teacher model that will be used to train our smaller student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ab8b3",
   "metadata": {
    "gather": {
     "logged": 1745565289389
    }
   },
   "outputs": [],
   "source": [
    "# Read the JSONL file and process each question\n",
    "import os\n",
    "results = []\n",
    "\n",
    "# Ensure we're using the correct file path\n",
    "train_data_path = \"./data/train_original_data.jsonl\"\n",
    "\n",
    "# Check if the file exists and has content before proceeding\n",
    "if not os.path.exists(train_data_path):\n",
    "    print(f\"Error: File {train_data_path} does not exist! Please run the previous cells to create it.\")\n",
    "else:\n",
    "    file_size = os.path.getsize(train_data_path)\n",
    "    if file_size == 0:\n",
    "        print(f\"Warning: File {train_data_path} exists but is empty (0 bytes)! Please run the data generation cells first.\")\n",
    "    else:\n",
    "        print(f\"File {train_data_path} exists and has {file_size} bytes.\")\n",
    "        \n",
    "        # Validate first line to ensure it's valid JSON\n",
    "        with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "            first_line = file.readline().strip()\n",
    "            if not first_line:\n",
    "                print(\"Error: File exists but first line is empty!\")\n",
    "            else:\n",
    "                try:\n",
    "                    # Try parsing the first JSON line\n",
    "                    test_json = json.loads(first_line)\n",
    "                    print(\"JSON validation: First line is valid JSON\")\n",
    "                    \n",
    "                    # Continue with processing all lines\n",
    "                    file.seek(0)  # Go back to start of file\n",
    "                    total_lines = sum(1 for _ in file if _.strip())\n",
    "                    \n",
    "                    print(f\"Processing {total_lines} questions from {train_data_path}\")\n",
    "                    file.seek(0)  # Go back to start of file again\n",
    "                    \n",
    "                    # Initialize tqdm progress bar\n",
    "                    progress_bar = tqdm(total=total_lines, desc=\"Processing questions\", unit=\"question\")\n",
    "                    \n",
    "                    processed_count = 0\n",
    "                    error_count = 0\n",
    "                    \n",
    "                    for i, line in enumerate(file):\n",
    "                        if line.strip():  # Skip empty lines\n",
    "                            try:\n",
    "                                question_data = json.loads(line)\n",
    "                                result = process_question(question_data)\n",
    "                                results.append(result)\n",
    "                                processed_count += 1\n",
    "                                \n",
    "                                # Update progress bar description with latest result\n",
    "                                progress_bar.set_description(f\"Latest answer: {result['response'][:20]}...\")\n",
    "                                progress_bar.update(1)\n",
    "                                \n",
    "                            except json.JSONDecodeError as e:\n",
    "                                error_count += 1\n",
    "                                print(f\"\\nError parsing line {i+1}: {str(e)}\")\n",
    "                                print(f\"Problematic line content: '{line[:100]}...'\")\n",
    "                                progress_bar.update(1)\n",
    "                            except Exception as e:\n",
    "                                error_count += 1\n",
    "                                print(f\"\\nError processing line {i+1}: {str(e)}\")\n",
    "                                progress_bar.update(1)\n",
    "                    \n",
    "                    progress_bar.close()\n",
    "                    \n",
    "                    print(f\"\\nProcessing complete: {processed_count} successful, {error_count} errors\")\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error: Invalid JSON in file. First line error: {str(e)}\")\n",
    "                    print(f\"First line content: '{first_line[:100]}...'\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error during file processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40cbf5e",
   "metadata": {},
   "source": [
    "## 18. Save the Teacher Model Responses\n",
    "\n",
    "Now that we've collected answers from our teacher model, we'll save them to a new file for training our student model. This cell:\n",
    "\n",
    "1. **Defines the output file path** for our processed training data\n",
    "\n",
    "2. **Creates a progress bar** to track the saving process\n",
    "\n",
    "3. **Formats each result** into a simpler structure with just the question and answer\n",
    "\n",
    "4. **Writes each example** as a line in our output JSON Lines file\n",
    "\n",
    "The resulting `train_data.jsonl` file will be the training data for our student model in the next notebook. This is the key output from our knowledge distillation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6de3fb",
   "metadata": {
    "gather": {
     "logged": 1745565289816
    }
   },
   "outputs": [],
   "source": [
    "output_file_path = \"./data/train_data.jsonl\"\n",
    "\n",
    "# Initialize tqdm progress bar for writing results\n",
    "print(f\"Writing {len(results)} processed questions to {output_file_path}\")\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    for result in tqdm(results, desc=\"Writing results\", unit=\"record\"):\n",
    "        # Extract just the question content (removing the instruction part)\n",
    "        question_text = result[\"question\"]\n",
    "        # if \"Question: \" in question_text:\n",
    "        #     question_text = question_text.split(\"Question: \")[1].split(\"\\nAnswer Choices:\")[0]\n",
    "\n",
    "        # Create the simplified output format\n",
    "        output_line = {\n",
    "            \"Question\": question_text,\n",
    "            \"Answer\": result[\"response\"]\n",
    "        }\n",
    "\n",
    "        # Write as JSONL (one JSON object per line)\n",
    "        f.write(json.dumps(output_line, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f7457",
   "metadata": {},
   "source": [
    "## 19. Install Visualization Libraries\n",
    "\n",
    "Before we can analyze our results visually, we need to install the `matplotlib` library. This popular Python library will allow us to create charts and graphs to better understand the data we've collected from the teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccbff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aaeaa3",
   "metadata": {},
   "source": [
    "## 20. Analyze Answer Distribution\n",
    "\n",
    "Now we'll analyze how the teacher model responded to our questions. This helps us understand the quality of our training data and identify any potential issues. This cell:\n",
    "\n",
    "1. **Counts the frequency** of each answer choice (A, B, C, D, E)\n",
    "\n",
    "2. **Identifies non-standard responses** (anything other than a single letter)\n",
    "\n",
    "3. **Creates a bar chart** showing the distribution of answers\n",
    "\n",
    "4. **Calculates the success rate** - what percentage of responses were valid letter choices\n",
    "\n",
    "5. **Analyzes problematic responses** if any were found\n",
    "\n",
    "This analysis helps us ensure that our teacher model is providing high-quality, consistent responses that will be good training examples for our student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00c5f2",
   "metadata": {
    "gather": {
     "logged": 1745565290007
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the distillation results\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Count occurrences of each answer\n",
    "answer_counts = {}\n",
    "other_responses = []\n",
    "\n",
    "for result in results:\n",
    "    answer = result['response'].strip()\n",
    "    \n",
    "    # Try to extract just the letter if there's additional text\n",
    "    # This regex matches: A, A., A), (A), (A) text\n",
    "    match = re.search(r'(?:^|\\(|\\s)([A-E])(?:\\)|\\.|\\s|$)', answer, re.IGNORECASE)\n",
    "    if match:\n",
    "        answer = match.group(1).upper()  # Extract just the letter and convert to uppercase\n",
    "        answer_counts[answer] = answer_counts.get(answer, 0) + 1\n",
    "    else:\n",
    "        # For non-standard answers\n",
    "        answer_counts['Other'] = answer_counts.get('Other', 0) + 1\n",
    "        # Store problematic responses for analysis\n",
    "        other_responses.append(answer)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df = pd.DataFrame(list(answer_counts.items()), columns=['Answer', 'Count'])\n",
    "df = df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(df['Answer'], df['Count'], color='skyblue')\n",
    "plt.title('Distribution of Answers in Distilled Dataset')\n",
    "plt.xlabel('Answer Choice')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{height}', ha='center', va='bottom')\n",
    "\n",
    "# Calculate and display success rate (assuming valid answers are A-E)\n",
    "total_questions = sum(answer_counts.values())\n",
    "valid_answers = sum(answer_counts.get(ans, 0) for ans in ['A', 'B', 'C', 'D', 'E'])\n",
    "success_rate = (valid_answers / total_questions) * 100 if total_questions > 0 else 0\n",
    "\n",
    "plt.figtext(0.5, 0.01, f'Success Rate: {success_rate:.1f}% ({valid_answers}/{total_questions} questions with valid answers)', \n",
    "           ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Display sample of 'Other' responses if they exist\n",
    "if other_responses:\n",
    "    print(f\"\\nSamples of 'Other' responses (showing up to 10):\")\n",
    "    for i, resp in enumerate(other_responses[:10]):\n",
    "        print(f\"  {i+1}. '{resp}'\")\n",
    "    \n",
    "    # Analyze if there are patterns in the 'Other' responses\n",
    "    lowercase_letters = sum(1 for r in other_responses if r.lower() in ['a', 'b', 'c', 'd', 'e'])\n",
    "    has_period = sum(1 for r in other_responses if re.search(r'[A-Ea-e]\\.', r))\n",
    "    has_explanation = sum(1 for r in other_responses if len(r) > 5)  # Simple heuristic for explanations\n",
    "    \n",
    "    print(f\"\\nAnalysis of {len(other_responses)} 'Other' responses:\")\n",
    "    print(f\"  - Lowercase letters (a-e): {lowercase_letters} ({lowercase_letters/len(other_responses)*100:.1f}%)\")\n",
    "    print(f\"  - Answers with periods (A.): {has_period} ({has_period/len(other_responses)*100:.1f}%)\")\n",
    "    print(f\"  - Likely explanations (>5 chars): {has_explanation} ({has_explanation/len(other_responses)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c828d2",
   "metadata": {},
   "source": [
    "## 21. Detect Potential Model Bias\n",
    "\n",
    "This cell analyzes whether the teacher model shows any bias toward specific answer choices. In an ideal world, the multiple-choice answers would be evenly distributed (about 20% for each choice A-E). Here's what we're doing:\n",
    "\n",
    "1. **Calculate the expected distribution** assuming a uniform 20% for each answer choice\n",
    "\n",
    "2. **Compare the actual counts** to what we would expect in an unbiased dataset\n",
    "\n",
    "3. **Compute the percentage difference** between expected and actual distributions\n",
    "\n",
    "4. **Visualize the comparison** with a bar chart showing actual vs. expected counts\n",
    "\n",
    "This analysis helps identify if the teacher model has any systematic preferences for certain answer choices, which could potentially be transferred to the student model. Significant bias might require additional data processing or model adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be286dac",
   "metadata": {
    "gather": {
     "logged": 1745565290281
    }
   },
   "outputs": [],
   "source": [
    "# Analyze potential model bias in answer distribution\n",
    "\n",
    "# Expected distribution (ideally uniform for multiple choice)\n",
    "expected_prob = 0.2  # 20% chance for each of A,B,C,D,E in a uniform distribution\n",
    "expected_counts = {letter: total_questions * expected_prob for letter in ['A', 'B', 'C', 'D', 'E']}\n",
    "\n",
    "# Create a DataFrame for comparing actual vs expected\n",
    "comparison_data = []\n",
    "for letter in ['A', 'B', 'C', 'D', 'E']:\n",
    "    actual = answer_counts.get(letter, 0)\n",
    "    expected = expected_counts[letter]\n",
    "    difference = actual - expected\n",
    "    percent_diff = (difference / expected) * 100 if expected > 0 else 0\n",
    "    comparison_data.append({\n",
    "        'Answer': letter,\n",
    "        'Actual Count': actual,\n",
    "        'Expected Count': expected,\n",
    "        'Difference': difference,\n",
    "        'Percent Difference': percent_diff\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nAnalysis of potential answer bias:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Create a visual comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], comparison_df['Actual Count'], width, label='Actual', color='skyblue')\n",
    "plt.bar([i + width/2 for i in x], comparison_df['Expected Count'], width, label='Expected', color='lightgreen')\n",
    "\n",
    "plt.xlabel('Answer Choice')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Actual vs Expected Answer Distribution')\n",
    "plt.xticks(x, comparison_df['Answer'])\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84760413",
   "metadata": {},
   "source": [
    "## 22. Install Advanced Analysis Libraries\n",
    "\n",
    "For more sophisticated analysis of our results, we'll install additional libraries:\n",
    "\n",
    "- **seaborn**: An enhanced statistical data visualization library built on matplotlib\n",
    "- **scikit-learn**: A machine learning library that includes tools for data analysis and metrics\n",
    "\n",
    "These libraries will allow us to create confusion matrices and other advanced visualizations to better understand patterns in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26784578",
   "metadata": {},
   "source": [
    "## 23. Create Distribution Bias Matrix\n",
    "\n",
    "This cell creates an advanced visualization to analyze potential bias in how the teacher model answers questions. Since we don't have ground truth labels for our questions, we use an alternative approach:\n",
    "\n",
    "1. **Create a pseudo-confusion matrix** where:\n",
    "   - The diagonal shows how often the model selects each answer choice\n",
    "   - In an unbiased model, all diagonal values would be around 0.2 (20%)\n",
    "\n",
    "2. **Visualize using a heatmap** with color intensity showing the proportion of each answer\n",
    "\n",
    "3. **Add annotations** explaining how to interpret the matrix\n",
    "\n",
    "This analysis helps us identify if the teacher model systematically favors certain answer choices, which could affect the knowledge transferred to the student model. Significant imbalances might indicate bias that should be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e318ccd-9825-4b21-a321-89bb5bdfbc22",
   "metadata": {
    "gather": {
     "logged": 1745565290522
    }
   },
   "outputs": [],
   "source": [
    "# Create a confusion matrix to analyze answer patterns\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# We don't have ground truth labels in this dataset, but we can:\n",
    "# 1. Analyze confusion between expected uniform distribution and actual distribution\n",
    "# 2. Alternatively, check if there are patterns in how the model responds to different question types\n",
    "\n",
    "# Approach 1: Create a \"pseudo-confusion matrix\" showing bias toward certain answers\n",
    "# Normalize the counts to get proportions\n",
    "total_valid_answers = sum(answer_counts.get(ans, 0) for ans in ['A', 'B', 'C', 'D', 'E'])\n",
    "pseudo_cm = np.zeros((5, 5))\n",
    "\n",
    "# Fill the diagonal with actual proportions (representing how much the model prefers each answer)\n",
    "for i, letter in enumerate(['A', 'B', 'C', 'D', 'E']):\n",
    "    actual_prop = answer_counts.get(letter, 0) / total_valid_answers if total_valid_answers > 0 else 0\n",
    "    expected_prop = 0.2  # Expected uniform distribution (20% each)\n",
    "    \n",
    "    # The diagonal shows the actual proportion\n",
    "    pseudo_cm[i, i] = actual_prop\n",
    "    \n",
    "    # The off-diagonal elements represent the \"confusion\" - the difference between\n",
    "    # expected and actual distribution\n",
    "    for j in range(5):\n",
    "        if i != j:\n",
    "            pseudo_cm[i, j] = (1 - actual_prop) / 4  # Distribute remaining probability\n",
    "\n",
    "# Plot the pseudo-confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pseudo_cm, annot=True, fmt='.3f', cmap='Blues',\n",
    "           xticklabels=['A', 'B', 'C', 'D', 'E'],\n",
    "           yticklabels=['A', 'B', 'C', 'D', 'E'])\n",
    "plt.title('Answer Distribution Bias Matrix')\n",
    "plt.xlabel('Predicted Answer')\n",
    "plt.ylabel('Expected Uniform Distribution')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add text annotation explaining this visualization\n",
    "plt.figtext(0.5, 0.01, \n",
    "            'This matrix shows model bias toward certain answers.\\n'\n",
    "            'Diagonal values represent the proportion of each answer in the results.\\n'\n",
    "            'In an unbiased model, all diagonal values would be close to 0.2 (20%)',\n",
    "            ha='center', fontsize=11, bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Approach 2: If we had access to ground truth or additional features about questions,\n",
    "# we could create an actual confusion matrix here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41839376-61d5-498d-b575-d702356e1521",
   "metadata": {
    "gather": {
     "logged": 1745565315423
    }
   },
   "outputs": [],
   "source": [
    "# Creating a real confusion matrix using ground truth data\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the ground truth data\n",
    "train_data_path = \"./data/train_data.jsonl\"\n",
    "\n",
    "# Arrays to store ground truth and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Load and parse the data\n",
    "print(\"Loading ground truth data from\", train_data_path)\n",
    "with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # Extract the predicted answer from our results variable\n",
    "                question_text = data['Question']\n",
    "                model_answer = data['Answer'].strip()\n",
    "                \n",
    "                # Find the ground truth for this question by parsing the answer choices\n",
    "                # For simplicity, we'll use the letter of the answer\n",
    "                match = re.search(r'\\(([A-E])\\)', model_answer)\n",
    "                if match:\n",
    "                    predicted_letter = match.group(1).upper()\n",
    "                    predicted_labels.append(predicted_letter)\n",
    "                    \n",
    "                    # Try to extract what the actual ground truth is\n",
    "                    # Since we don't have explicit ground truth,\n",
    "                    # we'll use the consistent answer pattern from the commonsense_qa dataset\n",
    "                    # where the correct answer is part of the answer choices\n",
    "                    \n",
    "                    # For this demonstration, we will assume these answers are correct\n",
    "                    # In a real scenario, you would need to match against the ground truth answer key\n",
    "                    true_labels.append(predicted_letter)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line: {str(e)}\")\n",
    "\n",
    "# Since we don't have separate ground truth labels in this dataset,\n",
    "# we'll create a more advanced analysis by grouping answers by question type\n",
    "\n",
    "# 1. Create a simple question type classifier based on the first word of the question\n",
    "question_types = []\n",
    "question_type_predicted = []\n",
    "\n",
    "with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                question_text = data['Question']\n",
    "                \n",
    "                # Extract the actual question from the format\n",
    "                if \"Question: \" in question_text:\n",
    "                    actual_question = question_text.split(\"Question: \")[1].split(\"\\nAnswer Choices:\")[0]\n",
    "                    \n",
    "                    # Simple question type classification based on first word\n",
    "                    first_word = actual_question.strip().split()[0].lower()\n",
    "                    \n",
    "                    # Group question types\n",
    "                    if first_word in ['what', 'which']:\n",
    "                        q_type = 'What/Which'\n",
    "                    elif first_word in ['where']:\n",
    "                        q_type = 'Where'\n",
    "                    elif first_word in ['who']:\n",
    "                        q_type = 'Who'\n",
    "                    elif first_word in ['how']:\n",
    "                        q_type = 'How'\n",
    "                    elif first_word in ['why']:\n",
    "                        q_type = 'Why'\n",
    "                    else:\n",
    "                        q_type = 'Other'\n",
    "                    \n",
    "                    question_types.append(q_type)\n",
    "                    \n",
    "                    # Get the predicted answer letter\n",
    "                    model_answer = data['Answer'].strip()\n",
    "                    match = re.search(r'\\(([A-E])\\)', model_answer)\n",
    "                    if match:\n",
    "                        letter = match.group(1).upper()\n",
    "                        question_type_predicted.append(letter)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Count occurrences of each answer by question type\n",
    "q_type_counts = {}\n",
    "for q_type, pred in zip(question_types, question_type_predicted):\n",
    "    if q_type not in q_type_counts:\n",
    "        q_type_counts[q_type] = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}\n",
    "    \n",
    "    q_type_counts[q_type][pred] += 1\n",
    "\n",
    "# Create a better confusion matrix - question type vs answer choice\n",
    "unique_types = sorted(set(question_types))\n",
    "q_type_matrix = np.zeros((len(unique_types), 5))\n",
    "\n",
    "for i, q_type in enumerate(unique_types):\n",
    "    for j, letter in enumerate(['A', 'B', 'C', 'D', 'E']):\n",
    "        q_type_matrix[i, j] = q_type_counts.get(q_type, {}).get(letter, 0)\n",
    "\n",
    "# Normalize by row (question type) to get distribution\n",
    "row_sums = q_type_matrix.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "q_type_matrix_norm = q_type_matrix / row_sums\n",
    "\n",
    "# Plot the question type vs answer choice matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(q_type_matrix_norm, annot=True, fmt='.2f', cmap='YlGnBu',\n",
    "           xticklabels=['A', 'B', 'C', 'D', 'E'],\n",
    "           yticklabels=unique_types)\n",
    "plt.title('Answer Distribution by Question Type (Normalized)')\n",
    "plt.xlabel('Answer Choice')\n",
    "plt.ylabel('Question Type')\n",
    "\n",
    "# Add an explanation of the visualization\n",
    "plt.figtext(0.5, 0.01, \n",
    "            'This matrix shows how answer patterns vary by question type.\\n'\n",
    "            'Each row shows the distribution of answers for a specific question type.\\n'\n",
    "            'An unbiased model would show similar distributions across question types.',\n",
    "            ha='center', fontsize=11, bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Create a second visualization - pattern analysis by question length\n",
    "# Group questions by length and analyze answer patterns\n",
    "question_lengths = []\n",
    "length_predicted = []\n",
    "\n",
    "with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                question_text = data['Question']\n",
    "                \n",
    "                # Extract the actual question\n",
    "                if \"Question: \" in question_text:\n",
    "                    actual_question = question_text.split(\"Question: \")[1].split(\"\\nAnswer Choices:\")[0]\n",
    "                    \n",
    "                    # Get question length in words\n",
    "                    word_count = len(actual_question.split())\n",
    "                    \n",
    "                    # Group by length\n",
    "                    if word_count < 10:\n",
    "                        length_group = 'Very Short (<10 words)'\n",
    "                    elif word_count < 15:\n",
    "                        length_group = 'Short (10-14 words)'\n",
    "                    elif word_count < 20:\n",
    "                        length_group = 'Medium (15-19 words)'\n",
    "                    else:\n",
    "                        length_group = 'Long (20+ words)'\n",
    "                    \n",
    "                    question_lengths.append(length_group)\n",
    "                    \n",
    "                    # Get the predicted answer letter\n",
    "                    model_answer = data['Answer'].strip()\n",
    "                    match = re.search(r'\\(([A-E])\\)', model_answer)\n",
    "                    if match:\n",
    "                        letter = match.group(1).upper()\n",
    "                        length_predicted.append(letter)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Count occurrences by length group\n",
    "length_counts = {}\n",
    "for length, pred in zip(question_lengths, length_predicted):\n",
    "    if length not in length_counts:\n",
    "        length_counts[length] = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}\n",
    "    \n",
    "    length_counts[length][pred] += 1\n",
    "\n",
    "# Create matrix for length vs answer\n",
    "unique_lengths = ['Very Short (<10 words)', 'Short (10-14 words)', \n",
    "                 'Medium (15-19 words)', 'Long (20+ words)']\n",
    "unique_lengths = [l for l in unique_lengths if l in length_counts]\n",
    "length_matrix = np.zeros((len(unique_lengths), 5))\n",
    "\n",
    "for i, length in enumerate(unique_lengths):\n",
    "    for j, letter in enumerate(['A', 'B', 'C', 'D', 'E']):\n",
    "        length_matrix[i, j] = length_counts.get(length, {}).get(letter, 0)\n",
    "\n",
    "# Normalize by row\n",
    "row_sums = length_matrix.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "length_matrix_norm = length_matrix / row_sums\n",
    "\n",
    "# Plot the question length vs answer choice matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(length_matrix_norm, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "           xticklabels=['A', 'B', 'C', 'D', 'E'],\n",
    "           yticklabels=unique_lengths)\n",
    "plt.title('Answer Distribution by Question Length (Normalized)')\n",
    "plt.xlabel('Answer Choice')\n",
    "plt.ylabel('Question Length Group')\n",
    "\n",
    "# Add an explanation of the visualization\n",
    "plt.figtext(0.5, 0.01, \n",
    "            'This matrix shows how answer patterns vary by question length.\\n'\n",
    "            'Differences between rows may indicate biases in how the model handles questions of different complexity.',\n",
    "            ha='center', fontsize=11, bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
