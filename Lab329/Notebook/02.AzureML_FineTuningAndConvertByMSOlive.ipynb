{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf944c2",
   "metadata": {},
   "source": [
    "# Fine-tuning and Optimizing a Student Model\n",
    "\n",
    "This notebook demonstrates how to fine-tune a small \"student\" model using the training data we generated from our \"teacher\" model in the previous notebook. We'll also optimize the model for efficient deployment.\n",
    "\n",
    "![](../../lab_manual/images/step-2.png)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to use Microsoft Olive to fine-tune the Phi-4-mini model\n",
    "- How to apply Low-Rank Adaptation (LoRA) for efficient fine-tuning\n",
    "- How to convert a model to ONNX format for optimization\n",
    "- How to apply quantization to reduce model size\n",
    "- How to prepare the model for deployment on resource-constrained environments\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed the previous notebook (`01.AzureML_Distillation.ipynb`)\n",
    "- Generated training data in `data/train_data.jsonl`\n",
    "- Access to Azure ML with the Phi-4-mini model in the registry\n",
    "- Python environment with necessary libraries (which we'll install)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Azure Authentication**: Ensure you're logged in to Azure using `az login --use-device-code` in a terminal\n",
    "2. **Kernel Selection**: Change the Jupyter kernel to **\"Python 3.10 PyTorch and Tensorflow\"** using the selector in the top right\n",
    "3. **Environment File**: Ensure your `local.env` file exists with proper credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f080269",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "Before we begin, make sure you've completed these steps:\n",
    "\n",
    "1. **Azure Login**: Run `az login --use-device-code` in a terminal to authenticate with Azure\n",
    "\n",
    "2. **Kernel Selection**: Select the \"Python 3.10 PyTorch and Tensorflow\" kernel from the dropdown in the top-right corner. This kernel has most of the dependencies we need pre-installed.\n",
    "\n",
    "3. **Check Environment**: Ensure your `local.env` file is in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d712c",
   "metadata": {},
   "source": [
    "## 1. Install Authentication Packages\n",
    "\n",
    "Here we install the packages needed to authenticate with Azure services:\n",
    "\n",
    "- **azure-ai-ml**: The Azure ML SDK for working with Azure Machine Learning\n",
    "\n",
    "The `-U` flag ensures we get the latest versions of these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d617742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for authentication\n",
    "! pip install azure-ai-ml -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b941ef",
   "metadata": {},
   "source": [
    "## 2. Install PyTorch\n",
    "\n",
    "Here we install PyTorch, which is the deep learning framework we'll use for fine-tuning. This command installs\n",
    " \n",
    "- **torch**: The core PyTorch library for neural networks and tensor operations\n",
    "- **torchvision**: For computer vision tasks (included as a dependency)\n",
    "- **torchaudio**: For audio processing tasks (included as a dependency)\n",
    "    \n",
    "We're installing from a specific URL (`download.pytorch.org/whl/cu124`) to get a version compatible with CUDA 12.4, which is optimized for modern NVIDIA GPUs. The `-U` flag ensures we get the latest version.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385440eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip  install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43939308",
   "metadata": {},
   "source": [
    "## 3. Install Microsoft Olive\n",
    "\n",
    "Now we install Microsoft Olive, an open-source model optimization toolkit that will be the main tool for our fine-tuning and optimization process. The `[auto-opt]` option includes additional dependencies for automatic optimization.\n",
    "\n",
    "Olive provides:\n",
    "- Model fine-tuning capabilities\n",
    "- ONNX conversion tools\n",
    "- Quantization for model compression\n",
    "- Performance optimization for various hardware targets\n",
    "\n",
    "This powerful tool will help us efficiently fine-tune our model and prepare it for deployment on resource-constrained devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install olive-ai[auto-opt] -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc5c13",
   "metadata": {},
   "source": [
    "## 4. Verify Olive Installation\n",
    "\n",
    "We'll now check the installed version of Olive to ensure it installed correctly. This command shows:\n",
    "- The package name\n",
    "- The installed version number\n",
    "- Where the package is installed\n",
    "- The package's dependencies\n",
    "\n",
    "Confirming the version is important as different versions of Olive may have different features or requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08355c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip show olive-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a3bdf",
   "metadata": {},
   "source": [
    "## 5. Install ONNX Runtime GenAI\n",
    "\n",
    "Next, we install ONNX Runtime GenAI, a specialized version of ONNX Runtime designed specifically for generative AI models. This package will allow us to:\n",
    "\n",
    "- Run our optimized model efficiently\n",
    "- Leverage specialized optimizations for transformer models\n",
    "- Access adapter-based fine-tuning capabilities\n",
    "\n",
    "We're installing version 0.7.1 with the `--pre` flag because it's a pre-release version with features we need for our work. Later notebooks will use this to run inference with our optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69907a09",
   "metadata": {
    "gather": {
     "logged": 1744963274226
    }
   },
   "outputs": [],
   "source": [
    "! pip install onnxruntime-genai==0.7.1 --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2976cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install onnxruntime==1.21.1 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d1dc8",
   "metadata": {},
   "source": [
    "## 6. Package Management\n",
    "\n",
    "The next few cells handle package management to avoid conflicts. We're:\n",
    "\n",
    "1. **Uninstalling onnxruntime-gpu** to avoid conflicts with the regular onnxruntime package\n",
    "2. **Installing regular onnxruntime** for CPU-based inference\n",
    "3. **Installing additional dependencies** including:\n",
    "   - bitsandbytes: For efficient quantization\n",
    "   - transformers: For working with transformer models\n",
    "   - peft: For parameter-efficient fine-tuning (LoRA)\n",
    "   - accelerate: For optimized training\n",
    "\n",
    "These packages will ensure our environment is properly set up for fine-tuning and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba394391",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall onnxruntime-gpu --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c74e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers==4.49.0 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-ai-ml -U  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install marshmallow==3.23.2 -U   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b21c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy==1.23.5 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eed001",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ca04b-6242-42c7-8dff-3d5f8b1122b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a60ad",
   "metadata": {},
   "source": [
    "## 7. Fine-tune with Low-Rank Adaptation (LoRA)\n",
    "\n",
    "This is the core command that fine-tunes our student model. We're using Microsoft Olive's fine-tuning capabilities with LoRA (Low-Rank Adaptation), a parameter-efficient fine-tuning method. Here's what each parameter does:\n",
    "\n",
    "- **`--method lora`**: Use Low-Rank Adaptation, which adds small trainable matrices to key layers instead of updating all weights\n",
    "\n",
    "- **`--model_name_or_path`**: The base model to fine-tune (Phi-4-mini-instruct from Azure ML registry)\n",
    "\n",
    "- **`--trust_remote_code`**: Allow execution of code from the remote model repository\n",
    "\n",
    "- **`--data_name json`**: The format of our training data (JSON)\n",
    "\n",
    "- **`--data_files`**: Path to our training data generated from the teacher model\n",
    "\n",
    "- **`--text_template`**: Template for formatting inputs and outputs during training\n",
    "\n",
    "- **`--max_steps 100`**: Only train for 100 steps (for speed, in production you'd use more)\n",
    "\n",
    "- **`--output_path`**: Where to save the fine-tuned model and adapter\n",
    "\n",
    "- **`--target_modules`**: Which layers to apply LoRA to (attention and feed-forward layers)\n",
    "\n",
    "- **`--log_level 1`**: Set verbosity of logging\n",
    "\n",
    "This process will take several minutes to complete. It creates a LoRA adapter that captures the knowledge our model learned from the teacher without modifying the base model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22241278",
   "metadata": {
    "gather": {
     "logged": 1744964309865
    }
   },
   "outputs": [],
   "source": [
    "! olive finetune \\\n",
    "    --method lora \\\n",
    "    --model_name_or_path  azureml://registries/azureml/models/Phi-4-mini-instruct/versions/1 \\\n",
    "    --trust_remote_code \\\n",
    "    --data_name json \\\n",
    "    --data_files ./data/train_data.jsonl \\\n",
    "    --text_template \"<|user|>{Question}<|end|><|assistant|>{Answer}<|end|>\" \\\n",
    "    --max_steps 100 \\\n",
    "    --output_path models/phi-4-mini/ft \\\n",
    "    --target_modules \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\" \\\n",
    "    --log_level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50cfd3",
   "metadata": {},
   "source": [
    "## 8. Reinstall ONNX Runtime GenAI\n",
    "\n",
    "Here we reinstall ONNX Runtime GenAI to ensure we have the correct version after all our package management. This is a precautionary step to make sure we have the version (0.7.1) needed for our model optimization in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d6cc0-db61-4613-bc89-42fa13c06440",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install onnxruntime-genai==0.7.1 --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd785577",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install onnxruntime==1.21.1 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    " ! pip install protobuf==3.20.3 -U "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521bfae",
   "metadata": {},
   "source": [
    "## 9. Optimize and Quantize the Model\n",
    "\n",
    "This command uses Microsoft Olive's auto-optimization capabilities to convert our fine-tuned model to ONNX format and apply int4 quantization. Here's what each parameter does:\n",
    "\n",
    "- **`--model_name_or_path`**: The base model from Azure ML registry\n",
    "\n",
    "- **`--adapter_path`**: Path to our LoRA adapter created in the previous step\n",
    "\n",
    "- **`--device cpu`**: Target CPU for optimization (you could also use cuda for GPU)\n",
    "\n",
    "- **`--provider CPUExecutionProvider`**: Use the CPU execution provider for ONNX Runtime\n",
    "\n",
    "- **`--use_model_builder`**: Use Olive's model builder for optimized conversion\n",
    "\n",
    "- **`--precision int4`**: Apply int4 quantization, which reduces model size by up to 75% compared to FP16\n",
    "\n",
    "- **`--output_path`**: Where to save the optimized model\n",
    "\n",
    "- **`--log_level 1`**: Set verbosity of logging\n",
    "\n",
    "The optimization process:\n",
    "1. Merges the base model with our LoRA adapter\n",
    "2. Converts to ONNX format, which is more efficient for inference\n",
    "3. Applies int4 quantization to dramatically reduce model size\n",
    "4. Optimizes the model for CPU inference\n",
    "\n",
    "This process will take several minutes to complete. The result will be a much smaller, more efficient model that can run on devices with limited resources while maintaining most of the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff7789",
   "metadata": {
    "gather": {
     "logged": 1744965032561
    }
   },
   "outputs": [],
   "source": [
    "! olive auto-opt \\\n",
    "    --model_name_or_path  azureml://registries/azureml/models/Phi-4-mini-instruct/versions/1 \\\n",
    "    --adapter_path models/phi-4-mini/ft/adapter \\\n",
    "    --device cpu \\\n",
    "    --provider CPUExecutionProvider \\\n",
    "    --use_model_builder \\\n",
    "    --precision int4 \\\n",
    "    --output_path models/phi-4-mini/onnx \\\n",
    "    --log_level 1"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
