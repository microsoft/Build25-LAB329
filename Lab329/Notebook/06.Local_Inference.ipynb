{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ebaef48",
   "metadata": {},
   "source": [
    "# Running Local Inference with Your Optimized Model\n",
    "\n",
    "This notebook demonstrates how to use your downloaded model for local inference. Now that you've completed the knowledge distillation, model optimization, and download process, you can run your efficient, fine-tuned model directly on your local machine.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to load your optimized ONNX model locally\n",
    "- How to load your LoRA adapter for specialized knowledge\n",
    "- How to format prompts and run inference\n",
    "- How to interpret and process the model's responses\n",
    "- How to run multiple examples and analyze performance\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed the previous notebooks:\n",
    "  - `01.AzureML_Distillation.ipynb` (generated training data)\n",
    "  - `02.AzureML_FineTuningAndConvertByMSOlive.ipynb` (fine-tuned and optimized the model)\n",
    "  - `03.AzureML_RuningByORTGenAI.ipynb` (tested the optimized model)\n",
    "  - `04.AzureML_RegisterToAzureML.ipynb` (registered your model to Azure ML)\n",
    "  - `05.Local_Download.ipynb` (downloaded the model locally)\n",
    "- Model files downloaded from Azure ML to your local machine\n",
    "- Python environment with necessary libraries (which we'll install)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Python Environment**: Ensure you have Python 3.10+ installed locally\n",
    "2. **Model Files**: Verify your model files from the previous download step are available\n",
    "3. **Libraries**: We'll install the required libraries in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160e19d",
   "metadata": {},
   "source": [
    "## Local Environment Setup\n",
    "\n",
    "This notebook is designed to run on your local machine rather than in Azure ML studio. Make sure that:\n",
    "\n",
    "1. You're running this notebook on the machine where you downloaded the model files\n",
    "2. You have Python 3.10+ installed on this machine\n",
    "3. You have sufficient disk space and memory for model loading and inference\n",
    "\n",
    "We'll start by installing the necessary packages for local inference using ONNX Runtime GenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60cd0d",
   "metadata": {},
   "source": [
    "## 1. Package Installation Helper\n",
    "\n",
    "First, we define a helper function that will manage package installation. This function:\n",
    "\n",
    "1. Attempts to import the package first to check if it's already installed\n",
    "2. If the package is already installed, displays a confirmation message\n",
    "3. If the package is not installed, uses pip to install it\n",
    "\n",
    "This approach makes the notebook more efficient by avoiding unnecessary reinstallation of packages that are already present in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package_name):\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        print(f\"✓ {package_name} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package_name])\n",
    "        print(f\"✓ {package_name} installed successfully\")\n",
    "\n",
    "# Install essential packages\n",
    "install_package('onnxruntime-genai')\n",
    "\n",
    "print(\"\\nAll required packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2a71c",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Set Up Model Path\n",
    "\n",
    "Now we import the required libraries and set up the path to our downloaded model:\n",
    "\n",
    "- **onnxruntime_genai (og)**: The specialized ONNX Runtime for generative AI models\n",
    "- **os**: For file and path operations\n",
    "- **time**: For measuring inference time\n",
    "\n",
    "We also define the path to our model files and verify that they exist. This verification step ensures we have the correct path before attempting to load the model, which helps prevent cryptic errors later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Update this path to the location where your model was downloaded\n",
    "# If you used the default settings in 05.Local_Download.ipynb, the path may look like:\n",
    "# \"./fine-tuning-phi-4-mini-onnx-int4-cpu/1\"\n",
    "model_path = \"./fine-tuning-phi-4-mini-onnx-int4-cpu/1/model\"\n",
    "\n",
    "# Verify the model files exist\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Model found at path: {model_path}\")\n",
    "    print(\"Model files:\")\n",
    "    for file in os.listdir(model_path):\n",
    "        print(f\" - {file}\")\n",
    "else:\n",
    "    print(f\"❌ Model not found at path: {model_path}\")\n",
    "    print(\"Please update the model_path variable to point to your downloaded model directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5556202",
   "metadata": {},
   "source": [
    "## 3. Load the Model and Adapter\n",
    "\n",
    "Now we'll load our optimized ONNX model and the LoRA adapter for inference:\n",
    "\n",
    "1. The base model is loaded first, which contains the optimized int4 quantized Phi-4-Mini model\n",
    "\n",
    "2. Then we create an Adapters container to manage our LoRA adapter\n",
    "\n",
    "3. We load the specific adapter that contains our fine-tuned knowledge for multiple-choice questions\n",
    "\n",
    "4. Finally, we create a tokenizer for converting text to tokens and back\n",
    "\n",
    "This step may take a moment as the model is loaded into memory. The model size is significantly smaller than the original due to our int4 quantization and ONNX optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Loading model...\")\n",
    "    model = og.Model(model_path)\n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "    \n",
    "    # Load the adapter for QA task\n",
    "    print(\"\\nLoading adapter...\")\n",
    "    adapters = og.Adapters(model)\n",
    "    adapter_path = os.path.join(model_path, \"adapter_weights.onnx_adapter\")\n",
    "    \n",
    "    if os.path.exists(adapter_path):\n",
    "        adapters.load(adapter_path, \"qa_choice\")\n",
    "        print(\"✓ Adapter loaded successfully!\")\n",
    "    else:\n",
    "        print(f\"❌ Adapter not found at path: {adapter_path}\")\n",
    "        # Try to find adapter files in the model directory\n",
    "        adapter_files = [f for f in os.listdir(model_path) if 'adapter' in f.lower()]\n",
    "        if adapter_files:\n",
    "            print(f\"Found potential adapter files: {adapter_files}\")\n",
    "            # Try the first one\n",
    "            adapters.load(os.path.join(model_path, adapter_files[0]), \"qa_choice\")\n",
    "            print(f\"✓ Loaded adapter: {adapter_files[0]}\")\n",
    "        else:\n",
    "            print(\"No adapter files found. Model may not perform as expected.\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3390a3e",
   "metadata": {},
   "source": [
    "## 4. Set Up Tokenizer and Generator\n",
    "\n",
    "Now we'll set up the components needed for text generation:\n",
    "\n",
    "1. **Tokenizer**: Converts text into token IDs that the model can understand\n",
    "\n",
    "2. **Tokenizer Stream**: Helps decode generated tokens back to text on-the-fly\n",
    "\n",
    "3. **Search Options**: Configuration for text generation, including:\n",
    "   - Maximum length of generated text\n",
    "   - Memory management settings\n",
    "\n",
    "4. **Generator Parameters**: Takes our search options and configures the generation process\n",
    "\n",
    "5. **Generator**: The object that will handle the actual token generation\n",
    "\n",
    "These components work together to handle the conversion between text and tokens, and to control how the model generates its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Set up tokenizer\n",
    "    print(\"Setting up tokenizer...\")\n",
    "    tokenizer = og.Tokenizer(model)\n",
    "    tokenizer_stream = tokenizer.create_stream()\n",
    "    \n",
    "    # Configure search options\n",
    "    search_options = {}\n",
    "    search_options['max_length'] = 102\n",
    "    search_options['past_present_share_buffer'] = False\n",
    "    search_options['repeat_penalty'] = 1.1\n",
    "    search_options['temperature'] = 0.7\n",
    "    \n",
    "    print(\"✓ Tokenizer and generator parameters configured!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error setting up tokenizer: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11ad1b",
   "metadata": {},
   "source": [
    "## Create a Function for Generating Responses\n",
    "\n",
    "Let's create a function that can generate responses for multiple-choice questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb9255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, choices):\n",
    "    \"\"\"\n",
    "    Generate a response to a multiple-choice question\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to answer\n",
    "        choices (dict): A dictionary where keys are choice labels (A, B, C...) and values are choice texts\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response (should be one of the choice labels)\n",
    "    \"\"\"\n",
    "    # Format the question with choices\n",
    "    choice_text = \"\\n\".join([f\"({label}) {text}\" for label, text in choices.items()])\n",
    "    input_text = f\"Answer the following multiple-choice question by selecting the correct option.\\n\\nQuestion: {question}\\nAnswer Choices:\\n{choice_text}\"\n",
    "    \n",
    "    # Format using the chat template\n",
    "    chat_template = \"</s>You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.<|end|><|user|>{input}<|end|><|assistant|>\"\n",
    "    prompt = chat_template.format(input=input_text)\n",
    "    \n",
    "    try:\n",
    "        # Print what we're sending to the model\n",
    "        print(f\"Generating response for question: \\\"{question[:50]}...\\\"\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Encode the input\n",
    "        input_tokens = tokenizer.encode(prompt)\n",
    "        \n",
    "        # Create generator parameters\n",
    "        params = og.GeneratorParams(model)\n",
    "        params.set_search_options(**search_options)\n",
    "        \n",
    "        # Create generator\n",
    "        generator = og.Generator(model, params)\n",
    "        \n",
    "        # Set the active adapter\n",
    "        generator.set_active_adapter(adapters, \"qa_choice\")\n",
    "        \n",
    "        # Generate tokens\n",
    "        generator.append_tokens(input_tokens)\n",
    "        \n",
    "        # Get the generated tokens\n",
    "        result = \"\"\n",
    "        for token in tokenizer_stream.output_tokens():\n",
    "            result += token\n",
    "            \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Clean and format the result to extract just the answer choice\n",
    "        result = result.strip()\n",
    "        \n",
    "        print(f\"Response generated in {(end_time - start_time):.2f} seconds\")\n",
    "        print(f\"Raw response: \\\"{result}\\\"\")\n",
    "        \n",
    "        # Try to find the answer choice (A, B, C, D, E) in the response\n",
    "        for choice in choices.keys():\n",
    "            if choice in result:\n",
    "                return choice\n",
    "                \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating response: {str(e)}\")\n",
    "        return \"Error: \" + str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83b9c2",
   "metadata": {},
   "source": [
    "## Test the Model with Example Questions\n",
    "\n",
    "Now let's test the model with some example multiple-choice questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50754f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some test questions\n",
    "test_questions = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"choices\": {\n",
    "            \"A\": \"Berlin\",\n",
    "            \"B\": \"London\",\n",
    "            \"C\": \"Paris\",\n",
    "            \"D\": \"Madrid\",\n",
    "            \"E\": \"Rome\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which planet is closest to the Sun?\",\n",
    "        \"choices\": {\n",
    "            \"A\": \"Venus\",\n",
    "            \"B\": \"Earth\",\n",
    "            \"C\": \"Mercury\",\n",
    "            \"D\": \"Mars\",\n",
    "            \"E\": \"Jupiter\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is 7 × 8?\",\n",
    "        \"choices\": {\n",
    "            \"A\": \"54\",\n",
    "            \"B\": \"56\",\n",
    "            \"C\": \"42\",\n",
    "            \"D\": \"64\",\n",
    "            \"E\": \"48\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate responses for each question\n",
    "for i, test_q in enumerate(test_questions):\n",
    "    print(f\"\\n--- Question {i+1} ---\")\n",
    "    response = generate_response(test_q[\"question\"], test_q[\"choices\"])\n",
    "    print(f\"Final answer: {response}\")\n",
    "    \n",
    "    # Check if the response is a valid choice\n",
    "    if response in test_q[\"choices\"]:\n",
    "        print(f\"Selected: {response}: {test_q['choices'][response]}\")\n",
    "    else:\n",
    "        print(f\"Response doesn't match any of the choices: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6c2fc",
   "metadata": {},
   "source": [
    "## Try Your Own Questions\n",
    "\n",
    "Now you can try your own multiple-choice questions with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, choices_dict):\n",
    "    \"\"\"\n",
    "    Ask the model a multiple-choice question with custom choices\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask\n",
    "        choices_dict (dict): A dictionary of choices (e.g., {\"A\": \"Option 1\", \"B\": \"Option 2\"})\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Custom Question ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"Choices:\")\n",
    "    for label, text in choices_dict.items():\n",
    "        print(f\" - {label}: {text}\")\n",
    "        \n",
    "    response = generate_response(question, choices_dict)\n",
    "    print(f\"\\nModel's answer: {response}\")\n",
    "    \n",
    "    if response in choices_dict:\n",
    "        print(f\"Selected: {response}: {choices_dict[response]}\")\n",
    "    else:\n",
    "        print(f\"Response doesn't match any of the choices: {response}\")\n",
    "\n",
    "# Example usage - try your own questions here:\n",
    "ask_question(\n",
    "    \"What is the main purpose of knowledge distillation in machine learning?\",\n",
    "    {\n",
    "        \"A\": \"To make models physically smaller in file size\",\n",
    "        \"B\": \"To transfer knowledge from larger models to smaller ones\",\n",
    "        \"C\": \"To increase the number of parameters in a model\",\n",
    "        \"D\": \"To make training data more compact\",\n",
    "        \"E\": \"To replace human knowledge with AI\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d003e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. Loaded your distilled and optimized Phi-4-mini model locally\n",
    "2. Created an inference pipeline using ONNX Runtime GenAI\n",
    "3. Tested the model with multiple-choice questions\n",
    "\n",
    "This demonstrates that your knowledge distillation process successfully created a smaller model that can run efficiently on local hardware while still providing intelligent responses.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try more complex questions or different formats\n",
    "- Benchmark the model's performance and memory usage\n",
    "- Integrate the model into your applications\n",
    "- Explore deploying the model on edge devices"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
