{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489906e9",
   "metadata": {},
   "source": [
    "# Local Inference with Azure Foundry Local\n",
    "\n",
    "**Placeholder** This notebook demonstrates how to use Azure Foundry Local to run inference with your optimized model on your local machine. Azure Foundry Local provides a simple, containerized way to serve and interact with large language models, including those you have fine-tuned and exported from Azure ML.\n",
    "\n",
    "![](../../lab_manual/images/step-4.png)\n",
    "\n",
    "## What You'll Learn\n",
    "- How to install and configure Azure Foundry Local\n",
    "- How to launch a local model server using Foundry\n",
    "- How to send prompts and receive completions from your model\n",
    "- How to use the Foundry Python SDK for local inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf94568",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "- Completed the previous notebooks and have a model exported in ONNX or supported format (see 05.Local_Download.ipynb)\n",
    "- Windows, macOS, or Linux\n",
    "- Python 3.10+ installed locally\n",
    "- Sufficient disk space and memory for your model\n",
    "\n",
    "## References\n",
    "- [Azure Foundry Local Documentation](https://github.com/microsoft/Foundry-Local/tree/main/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35152bf5",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Prepare Your Model and Config for Foundry Local\n",
    "\n",
    "- Ensure your model and any adapters (such as LoRA) are exported in a format supported by Foundry Local (e.g., ONNX, GGUF, or HuggingFace Transformers format).\n",
    "- You have successfully downloaded a onnx model from notebook 6. to `fine-tuning-phi-4-mini-onnx-int4-cpu`\n",
    "- We now need to copy the contents of the `model` folder `fine-tuning-phi-4-mini-onnx-int4-cpu` to the `LocalFoundryEnv/models` folder\n",
    "- Create or update an `inference_model.json` config file in the models directory, following the [Foundry Local model config guide](https://github.com/microsoft/Foundry-Local/blob/main/docs/model-config.md).\n",
    "\n",
    "Example `inference_model.json`:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Name\": \"phi-4-mini-reasoning-onnx\",\n",
    "    \"PromptTemplate\": {\n",
    "      \"assistant\": \"{Content}\",\n",
    "      \"prompt\": \"<|user|>Explain the Pythagorean Theorem<|end|><|assistant|>\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "> Tip: If you used 05.Local_Download.ipynb, your model files should already be in a suitable directory. Just add or edit the config file as above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52efbaa",
   "metadata": {},
   "source": [
    "## Open the intergrated terminal in VScode \n",
    "\n",
    "Right Click on the LocalFoundryEnv folder and select open in intergrated terminal.\n",
    "\n",
    "In VSCode you should now see a interfrated terminal with \n",
    "PS C:\\Users\\LabUser\\Desktop\\lab\\Build25-LAB329\\Lab329\\LocalFoundryEnv>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9f264",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Running Fine-tuning Model with Foundry Local\n",
    "\n",
    "\n",
    "```bash\n",
    "\n",
    "foundry cache cd models  \n",
    "\n",
    "foundry cache list\n",
    "\n",
    "foundry model run model --verbose \n",
    "\n",
    "```\n",
    "\n",
    "You can copy this question to answer\n",
    "\n",
    "\n",
    "```txt\n",
    "\n",
    "Answer the following multiple-choice question by selecting the correct option.\\n\\nQuestion: Sammy wanted to go to where the people were.  Where might he go?\\nAnswer Choices:\\n(A) race track\\n(B) populated areas\\n(C) the desert\\n(D) apartment\\n(E) roadblock\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d0cb1",
   "metadata": {},
   "source": [
    "## 5. Explore Foundry Local CLI commands\n",
    "The foundry CLI is structured into several categories:\n",
    "\n",
    "- Model: Commands related to managing and running models\n",
    "- Service: Commands for managing the AI Foundry Local service\n",
    "- Cache: Commands for managing the local cache where models are stored\n",
    "- To see all available commands, use the help option: `foundry --help`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f3e5f",
   "metadata": {},
   "source": [
    "## 6. Try Your Own Questions\n",
    "\n",
    "You can now use the `client` object to send any prompt to your local model. Try with your own multiple-choice questions or other tasks supported by your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af86a2d",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "- Explore more advanced prompt engineering and system instructions\n",
    "- Benchmark your model's performance locally\n",
    "- Integrate the local Foundry server into your applications\n",
    "- For more details, see the [Foundry Local documentation](https://github.com/microsoft/Foundry-Local/tree/main/docs)\n",
    "\n",
    "\n",
    "**Congratulations!** You have successfully run local inference with your optimized model using Azure Foundry Local.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
