# Step 7: Local Inference with Foundry Local

Deploy and test your optimized, distilled model on your local machine using Azure Foundry Local. This step demonstrates how to serve and interact with your model in a production-like, resource-constrained environment.

**Notebook:** 07.Local_inference_AIFoundry.ipynb

**Time:** 10 min

## Instructions

You will be running this notebook from the Skillable VM - **Not** from Azure ML Studio.

1. Open **Labs** folder found on the Desktop of your Skillable VM.
2. **Navigate** to the **Build25-LAB329\Lab329** folder for this lab.
3. **Open** the **07.Local_inference_AIFoundry.ipynb** Notebook.
4. **Read the purpose and overview** at the top of the notebook.
5. When you open the notebook, you'll see an instruction for **Kernel Selection**. It's **very important** to select the **correct kernel** for your environment.

We have also provided a downloaded version of the model in **C:\Users\LabUser\Desktop\lab\fine-tuning-phi-4-mini-onnx-int4-cpu**, accessible via the **lab** folder on the desktop.
