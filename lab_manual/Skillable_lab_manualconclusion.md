### Conclusion

## What You've Learned

Congratulations! You've successfully completed the Model Distillation Workshop. Here's what you've accomplished:

1. **Generated training data** using a large teacher model (DeepSeek-V3)
2. **Fine-tuned a smaller student model** (Phi-4-mini) using LoRA
3. **Optimized the model** with ONNX conversion and int4 quantization 
4. **Tested the model** using ONNX Runtime GenAI
5. **Registered the model** to Azure ML
6. **Downloaded and run the model locally**

You've learned:
- How knowledge distillation transfers intelligence from large to small models
- How to use Microsoft Olive for fine-tuning and optimization
- How to use ONNX Runtime GenAI for efficient inference
- How to deploy models locally for edge computing scenarios

## Next Steps

Here are some ways to build on what you've learned:

1. **Try different datasets**
   - Use different types of questions or tasks
   - Create your own custom dataset

2. **Explore different models**
   - Try different teacher models (GPT-4, Claude, etc.)
   - Try different student models (Phi-3, Llama, etc.)

3. **Optimize for different targets**
   - Try different quantization levels (int8, fp16)
   - Target different hardware (ARM, NVIDIA Jetson, etc.)

4. **Build applications**
   - Create a simple web UI for your model
   - Integrate it with other applications

5. **Learn more about:**
   - Microsoft Olive +++https://github.com/microsoft/Olive+++
   - ONNX Runtime +++https://onnxruntime.ai/+++
   - [Azure ML +++ https://learn.microsoft.com/azure/machine-learning+++

Thank you for participating in this workshop! Your feedback is valuable for improving future sessions. If you would like to retry this lab or access the resources these are available at Build Lab329 +++https://github.com/microsoft/Build25_Lab329+++
